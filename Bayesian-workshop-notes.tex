\documentclass[titlepage,landscape,pdftex]{seminar}
\usepackage{url}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
  pdftitle={Introduction to Bayesian inference for evolutionists \& ecologists}
  pdfauthor={Kent E. Holsinger, University of Connecticut}
  pdfpagemode={FullScreen},
  pdfborder={0 0 0}
}
\usepackage{epstopdf}

\slideframe{none}
\pagestyle{empty}

\newcommand{\heading}[1]{{\color{red}\large\bf#1}}

\title{\color{red}Introduction to Bayesian inference for evolutionists \& ecologists}
\author{Kent E. Holsinger \\
  Department of Ecology \& Evolutionary Biology, U-3043 \\
  University of Connecticut \\
  Storrs, CT 06269-3043}
\date{\copyright{} 2020 by Kent E. Holsinger}

\begin{document}

\begin{slide}

  \begin{center}\maketitle\end{center}

  \vfill

\end{slide}

\begin{slide}
  \heading{Why be Bayesian?}

  How worried should you be if you get a positive test for COVID-19?

  \begin{itemize}

  \item Abbott Alinity SARS-CoV2 Assay

    \begin{tabular}{lcc}
      \hline\hline
      SARS-CoV-2 concntration & Number tested & Number detected \\
      \hline
      1X to 2X LOD & 20 & 20 \\
      20X LOD & 20 & 20 \\
      Negative & 31 & 0 \\
      \hline
    \end{tabular}
    
  \item  Do you believe it's perfect?

    \begin{itemize}

    \item $\mbox{P}(\mbox{true positive}) = 0.976 (0.913, 1.000)$

    \item $\mbox{P}(\mbox{true negative}) = 0.970 (0.888, 0.999)$
      
    \end{itemize}
    
  \end{itemize}
  \vfill

\end{slide}

\begin{slide}
  \heading{Why be Bayesian?}

  \begin{itemize}
    
  \item Positive test $\ne$ carrying virus

    \begin{itemize}

    \item Positive and carrying virus (true positive)

    \item Positive and not carrying virus (false positive)

    \end{itemize}

  \item Assume prevalence (proportion of population carrying virus)
    is 3\% and imagine that we test 1000 people

  \end{itemize}

  \vfill
  
\end{slide}

\begin{slide}
  \heading{How much should I worry?}

  {\small
    \begin{eqnarray*}
      N_{\mbox{infected}} &=& 1000 \times 0.03  = 30 \\
      N_{\mbox{not infected}} & = & 1000 \times 0.97 = 970 \\
      N_{\mbox{infected and positive}} &=& N_{\mbox{infected}}\times
                                           0.976 \\
      N_{\mbox{not infected and positive}} &=&
                                               N_{\mbox{not infected}}\times
                                               0.030
    \end{eqnarray*}
  }

  \vfill

\end{slide}

\begin{slide}
  \heading{How much should I worry?}

  {\small
    \begin{eqnarray*}
      N_{\mbox{infected}} &=& 1000 \times 0.03  = 30 \\
      N_{\mbox{not infected}} & = & 1000 \times 0.97 = 970 \\
      N_{\mbox{infected and positive}} &=& N_{\mbox{infected}}\times
                                           0.976 \\
      N_{\mbox{not infected and positive}} &=&
        N_{\mbox{not infected}}\times 0.030 \\
      \frac{N_{\mbox{infected and positive}}}{N_{\mbox{{positive}}}} &=&
        \frac{30\times 0.976}{30\times 0.976 + 970\times 0.030} \\
      \\
        &=& \frac{29.3}{29.3 + 29.1} \\
      \\
        &=& 0.502
    \end{eqnarray*}
  }

  {\color{red}\bf
    Coin flip on whether you have COVID}
  \vfill
  
\end{slide}

\begin{slide}
  \heading{Bayes' Rule}

  \begin{eqnarray*}
  \mbox{P}(\mbox{infected}|\mbox{positive}) &=&
  \frac{\mbox{P}(\mbox{positive}|\mbox{infected})}
            {\mbox{P}(\mbox{positive})}\mbox{P}(\mbox{infected}) \\
    &=& \frac{0.976}{(0.976)(0.03) + (0.030)(0.97)}(0.03) \\
    &=& 0.502
    \end{eqnarray*}


  $$
  \mbox{P}(X|Y) = \frac{\mbox{P}(Y|X)}{\mbox{P}(X)}\mbox{P}(Y)
  $$

    \vfill

\end{slide}

\begin{slide}
  \heading{Bayes' Rule for inference}

  \begin{eqnarray*}
    \mbox{P}(\theta|X) &=&
      \frac{\mbox{P}(X|\theta)}{\mbox{P}(X)}\mbox{P}(\theta) \\
    \theta &=& \mbox{parameter} \\
    X &=& \mbox{data}             
  \end{eqnarray*}

  \vfill
  
\end{slide}

\begin{slide}
  \heading{Bayes' Rule for inference}

  \begin{eqnarray*}
    \mbox{P}(\theta|X) &=&
      \frac{\mbox{P}(X|\theta)}{\mbox{P}(X)}\mbox{P}(\theta) \\
    \theta &=& \mbox{parameter} \\
    X &=& \mbox{data} \\       
    \mbox{P}(X|\theta) &=& \mbox{likelihood}
  \end{eqnarray*}

\noindent {\color{red}Maximum likelihood estimate}: value of $\theta$ that
maximizes $\mbox{P}(X|\theta)$
  
  \vfill
  
\end{slide}

\begin{slide}
  \heading{Bayes' Rule for inference}

  \begin{eqnarray*}
    \mbox{P}(\theta|X) &=&
      \frac{\mbox{P}(X|\theta)}{\mbox{P}(X)}\mbox{P}(\theta) \\
    \theta &=& \mbox{parameter} \\
    X &=& \mbox{data} \\       
    \mbox{P}(X|\theta) &=& \mbox{likelihood} \\
    \mbox{P}(\theta) &=& \mbox{prior distribution of $\theta$} \\
    \mbox{P}(\theta|X) &=& \mbox{posterior distribution of $\theta$} 
  \end{eqnarray*}

  \noindent {\color{red}Bayesian inference}: based on posterior
  distribution, $\mbox{P}(\theta|X)$

  \vfill
  
\end{slide}

\begin{slide}
  \heading{Stan}

  \noindent A probabilistic language for Bayesian analysis

  {\tiny
\begin{verbatim}
data {
  int<lower=0> k;       // number of positives observed
  int<lower=0> N;       // number in sample
}

parameters {
  real<lower=0, upper=1> p;   // frequency of positives in the sample
}

model {
  // likelihood
  //
  k ~ binomial(N, p);

  // prior
  //
  p ~ uniform(0.0, 1.0);
}
\end{verbatim}
    }

\end{slide}

\begin{slide}
  \heading{Posterior comparisons}

  \noindent We have a posterior distribution of LMA in {\it Protea
    eximia\/} and {\it Protea punctata}

  \begin{enumerate}

    \item Set $i = 1$

    \item Take one sample at random from the posterior distribution of
      {\it Protea eximia\/}, one from the posterior distribution of
      {\it Protea punctata\/}, take the difference and record it as
      $\delta_i$.

    \item Increment $i$ by 1, and return to step 2 until $i$ is a
      ``big'' number.

    \item The set of $\delta_i$ you have is now a sample from the
      posterior distribution of the {\it difference}. If the 95\%
      credible interval is strictly positive, you have evidence that
      the LMA of {\it P. eximia\/} is bigger than the LMA of {\it
        P. punctata}. 

  \end{enumerate}

  \vfill
  
\end{slide}

\begin{slide}
  \heading{Scaling variables}

  \noindent {\tt scale()} sets mean to 0, standard deviation to 1

  \noindent Interpretation of regression coefficient:

  $$
  y_i = \beta_0 + \beta_1 x_i
  $$
  
  a 1-sd unit change in $x$ leads to a $\beta_1$-sd unit change in $y$

\end{slide}

\begin{slide}
  \heading{Horseshoe prior}

  \noindent Shrinkage factor: How much a regression coefficient is
  ``shrunk'' towards 0. Lies between 0 and 1.

  \begin{center}
    \includegraphics[width=0.6\textwidth]{horseshoe-prior.eps}
  \end{center}

  Pironen \& Vehtari, {\it Electronic Journal of Statistics}
  11:5018-5051; 2017.

\end{slide}

\begin{slide}
  \heading{Model choice}

\noindent {\bf Compromise between goodness of fit and number of parameters}

AIC:
\begin{eqnarray*}
  -2\log(\mbox{P}(X|\theta)) + 2k = D(X|\theta) + 2k
\end{eqnarray*}
DIC:
\begin{eqnarray*}
D(X|\hat\theta) + pD \\
pD &=& D(X|\hat\theta) - \widehat{D(X|\theta)}
\end{eqnarray*}

\vfill

\noindent {\bf Predictive ability}

WAIC (widely applicable information criterion): See Pironen \&
Vehtari, {\it Statistics and Computing} 27:711-735; 2017 for details

LOO-CV: equivalent to WAIC

\end{slide}

\begin{slide}
  \heading{Leave-one out cross-validation}

  \noindent Cross-validation: Leaving some data out of the model, then
  ``cross-validating'' the model by seeing how well you can predict
  the data you left out.

  \noindent Leave-one out cross-validation: Leave each observation out
  of the model, fit the model, predict the observation, and compare
  prediction to observed {\it for every observation in the data set}.

  \noindent {\tt loo()} provides a sneaky way of doing this while only
  fitting the model once.

\end{slide}

\end{document}
